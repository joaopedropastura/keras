{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaopedropastura/keras/blob/main/model_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ3Aak07spNl"
      },
      "outputs": [],
      "source": [
        "!pip install keras\n",
        "!pip install pandas\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hOK3Aldl7LHt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\disrct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import activations\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras import metrics\n",
        "from keras import losses\n",
        "from keras import callbacks\n",
        "from keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N0gCeYIexnGg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\disrct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = models.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KPQNDsnaxsaA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\disrct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.add(layers.Conv2D(\n",
        "    32, (5,5), # 32 a quantidade de filtros, (5x5) = tamanho do filtro gerando imagens 60x60\n",
        "    input_shape = (64, 64, 3), # 3 = 3 canais rgb, 64 = iamgens com o tamanho 64x64\n",
        "    activation = 'relu'\n",
        "))\n",
        "\n",
        "model.add(layers.MaxPooling2D(\n",
        "    pool_size = (2, 2) #subsalpling, vamos de 2 em 2 pegando o maior valor\n",
        "))\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    16, (5,5), # 16 a quantidade de filtros, (5x5) = tamanho do filtro gerando imagens 26x26\n",
        "    input_shape = (30, 30, 3), # 3 = 3 canais rgb, 30 = iamgens com o tamanho 30x30\n",
        "    activation = 'relu'\n",
        "))\n",
        "\n",
        "model.add(layers.MaxPooling2D(\n",
        "    pool_size = (2, 2) #subsalpling, vamos de 2 em 2 pegando o maior valor\n",
        "))\n",
        "\n",
        "# model.add(layers.Conv2D(\n",
        "#     8, (5,5), # 32 a quantidade de filtros, (5x5) = tamanho do filtro gerando imagens 60x60\n",
        "#     input_shape = (13, 13, 3), # 3 = 3 canais rgb, 64 = iamgens com o tamanho 64x64\n",
        "#     activation = 'relu'\n",
        "# ))\n",
        "\n",
        "# model.add(layers.MaxPooling2D(\n",
        "#     pool_size = (2, 2) #subsalpling, vamos de 2 em 2 pegando o maior valor\n",
        "# ))\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    4, (5,5), # 32 a quantidade de filtros, (5x5) = tamanho do filtro gerando imagens 60x60\n",
        "    input_shape = (13, 13, 3), # 3 = 3 canais rgb, 64 = iamgens com o tamanho 64x64\n",
        "    activation = 'relu'\n",
        "))\n",
        "\n",
        "model.add(layers.MaxPooling2D(\n",
        "    pool_size = (2, 2) #subsalpling, vamos de 2 em 2 pegando o maior valor\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aQwymx6u2pTQ"
      },
      "outputs": [],
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(\n",
        "    256, #\n",
        "    kernel_regularizer= regularizers.L2(1e-4),\n",
        "    kernel_initializer= initializers.RandomNormal(stddev = 1), #incializadores iniciando com valores maiores que um\n",
        "    bias_initializer= initializers.Zeros(),\n",
        "    activation=activations.relu\n",
        "))\n",
        "model.add(layers.Dropout(0.2)) #20% da camada anterior é desligada para evitar overfitting\n",
        "\n",
        "\n",
        "model.add(layers.Dense(\n",
        "    64, #\n",
        "    kernel_regularizer= regularizers.L2(1e-4),\n",
        "    kernel_initializer= initializers.RandomNormal(stddev = 1), #incializadores iniciando com valores maiores que um\n",
        "    bias_initializer= initializers.Zeros()\n",
        "))\n",
        "model.add(layers.Activation(activations.relu))\n",
        "#depois da camada de ativação\n",
        "\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(layers.Dense(\n",
        "    64, #\n",
        "    kernel_regularizer= regularizers.L2(1e-4),\n",
        "    kernel_initializer= initializers.RandomNormal(stddev = 1), #incializadores iniciando com valores maiores que um\n",
        "    bias_initializer= initializers.Zeros()\n",
        "))\n",
        "model.add(layers.Activation(activations.relu))\n",
        "\n",
        "model.add(layers.Dense(\n",
        "    2, #\n",
        "    kernel_regularizer= regularizers.L2(1e-4),\n",
        "    kernel_initializer= initializers.RandomNormal(stddev = 1), #incializadores iniciando com valores maiores que um\n",
        "    bias_initializer= initializers.Zeros()\n",
        "))\n",
        "model.add(layers.Activation(activations.softmax))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kbPq5xH4QSp3"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=optimizers.Adam(),\n",
        "    loss=losses.BinaryCrossentropy(),\n",
        "    metrics=[metrics.Accuracy(), metrics.Precision()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RNKzzn4zT5_d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 19996 images belonging to 2 classes.\n",
            "Found 4998 images belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "310/310 [==============================] - 34s 109ms/step - loss: 4.9596 - accuracy: 0.0191 - precision: 0.5146\n",
            "Epoch 2/50\n",
            "310/310 [==============================] - 39s 125ms/step - loss: 4.1890 - accuracy: 0.0082 - precision: 0.5169\n",
            "Epoch 3/50\n",
            "310/310 [==============================] - 40s 128ms/step - loss: 3.6568 - accuracy: 0.0027 - precision: 0.5054\n",
            "Epoch 4/50\n",
            "310/310 [==============================] - 40s 130ms/step - loss: 3.2548 - accuracy: 9.0891e-04 - precision: 0.5068\n",
            "Epoch 5/50\n",
            "310/310 [==============================] - 39s 126ms/step - loss: 2.9938 - accuracy: 4.0396e-04 - precision: 0.5078\n",
            "Epoch 6/50\n",
            "310/310 [==============================] - 38s 124ms/step - loss: 2.8161 - accuracy: 4.2921e-04 - precision: 0.5177\n",
            "Epoch 7/50\n",
            "310/310 [==============================] - 38s 122ms/step - loss: 2.7076 - accuracy: 2.5247e-04 - precision: 0.5107\n",
            "Epoch 8/50\n",
            "310/310 [==============================] - 39s 126ms/step - loss: 2.6039 - accuracy: 0.0000e+00 - precision: 0.5210\n",
            "Epoch 9/50\n",
            "310/310 [==============================] - 38s 123ms/step - loss: 2.5314 - accuracy: 7.5742e-05 - precision: 0.5214\n",
            "Epoch 10/50\n",
            "310/310 [==============================] - 39s 124ms/step - loss: 2.4693 - accuracy: 0.0000e+00 - precision: 0.5228\n",
            "Epoch 11/50\n",
            "310/310 [==============================] - 38s 123ms/step - loss: 2.4073 - accuracy: 0.0000e+00 - precision: 0.5189\n",
            "Epoch 12/50\n",
            "310/310 [==============================] - 38s 121ms/step - loss: 2.3471 - accuracy: 0.0000e+00 - precision: 0.5201\n",
            "Epoch 13/50\n",
            "310/310 [==============================] - 38s 122ms/step - loss: 2.2882 - accuracy: 0.0000e+00 - precision: 0.5241\n",
            "Epoch 14/50\n",
            "310/310 [==============================] - 37s 120ms/step - loss: 2.2270 - accuracy: 0.0000e+00 - precision: 0.5204\n",
            "Epoch 15/50\n",
            "310/310 [==============================] - 37s 118ms/step - loss: 2.1636 - accuracy: 0.0000e+00 - precision: 0.5269\n",
            "Epoch 16/50\n",
            "310/310 [==============================] - 37s 119ms/step - loss: 2.0994 - accuracy: 0.0000e+00 - precision: 0.5269\n",
            "Epoch 17/50\n",
            "310/310 [==============================] - 38s 122ms/step - loss: 2.0361 - accuracy: 0.0000e+00 - precision: 0.5276\n",
            "Epoch 18/50\n",
            "310/310 [==============================] - 37s 120ms/step - loss: 1.9709 - accuracy: 0.0000e+00 - precision: 0.5320\n",
            "Epoch 19/50\n",
            "310/310 [==============================] - 38s 121ms/step - loss: 1.9049 - accuracy: 0.0000e+00 - precision: 0.5338\n",
            "Epoch 20/50\n",
            "310/310 [==============================] - 37s 119ms/step - loss: 1.8396 - accuracy: 0.0000e+00 - precision: 0.5362\n",
            "Epoch 21/50\n",
            "310/310 [==============================] - 37s 121ms/step - loss: 1.7718 - accuracy: 0.0000e+00 - precision: 0.5438\n",
            "Epoch 22/50\n",
            "310/310 [==============================] - 37s 121ms/step - loss: 1.7072 - accuracy: 0.0000e+00 - precision: 0.5440\n",
            "Epoch 23/50\n",
            "310/310 [==============================] - 40s 128ms/step - loss: 1.6400 - accuracy: 0.0000e+00 - precision: 0.5537\n",
            "Epoch 24/50\n",
            "310/310 [==============================] - 38s 121ms/step - loss: 1.5756 - accuracy: 0.0000e+00 - precision: 0.5568\n",
            "Epoch 25/50\n",
            "310/310 [==============================] - 38s 122ms/step - loss: 1.5123 - accuracy: 0.0000e+00 - precision: 0.5683\n",
            "Epoch 26/50\n",
            "310/310 [==============================] - 38s 121ms/step - loss: 1.4523 - accuracy: 0.0000e+00 - precision: 0.5708\n",
            "Epoch 27/50\n",
            "310/310 [==============================] - 38s 123ms/step - loss: 1.3943 - accuracy: 0.0000e+00 - precision: 0.5808\n",
            "Epoch 28/50\n",
            "310/310 [==============================] - 38s 123ms/step - loss: 1.3398 - accuracy: 0.0000e+00 - precision: 0.5799\n",
            "Epoch 29/50\n",
            "310/310 [==============================] - 39s 125ms/step - loss: 1.2897 - accuracy: 0.0000e+00 - precision: 0.5842\n",
            "Epoch 30/50\n",
            "310/310 [==============================] - 54s 175ms/step - loss: 1.2467 - accuracy: 0.0000e+00 - precision: 0.5747\n",
            "Epoch 31/50\n",
            "310/310 [==============================] - 65s 209ms/step - loss: 1.2023 - accuracy: 0.0000e+00 - precision: 0.5762\n",
            "Epoch 32/50\n",
            "310/310 [==============================] - 55s 177ms/step - loss: 1.1598 - accuracy: 0.0000e+00 - precision: 0.5846\n",
            "Epoch 33/50\n",
            "310/310 [==============================] - 51s 166ms/step - loss: 1.1220 - accuracy: 0.0000e+00 - precision: 0.5845\n",
            "Epoch 34/50\n",
            "310/310 [==============================] - 50s 162ms/step - loss: 1.0883 - accuracy: 0.0000e+00 - precision: 0.5875\n",
            "Epoch 35/50\n",
            "310/310 [==============================] - 50s 160ms/step - loss: 1.0755 - accuracy: 0.0000e+00 - precision: 0.5513\n",
            "Epoch 36/50\n",
            "310/310 [==============================] - 51s 165ms/step - loss: 1.0446 - accuracy: 0.0000e+00 - precision: 0.5730\n",
            "Epoch 37/50\n",
            "310/310 [==============================] - 53s 170ms/step - loss: 1.0175 - accuracy: 0.0000e+00 - precision: 0.5804\n",
            "Epoch 38/50\n",
            "310/310 [==============================] - 56s 179ms/step - loss: 0.9830 - accuracy: 0.0000e+00 - precision: 0.6028\n",
            "Epoch 39/50\n",
            "310/310 [==============================] - 51s 164ms/step - loss: 0.9495 - accuracy: 0.0000e+00 - precision: 0.6248\n",
            "Epoch 40/50\n",
            "310/310 [==============================] - 46s 148ms/step - loss: 0.9637 - accuracy: 0.0000e+00 - precision: 0.5330\n",
            "Epoch 41/50\n",
            "310/310 [==============================] - 46s 149ms/step - loss: 0.9558 - accuracy: 0.0000e+00 - precision: 0.5089\n",
            "Epoch 42/50\n",
            "310/310 [==============================] - 45s 147ms/step - loss: 0.9363 - accuracy: 0.0000e+00 - precision: 0.5020\n",
            "Epoch 43/50\n",
            "310/310 [==============================] - 42s 135ms/step - loss: 0.9163 - accuracy: 0.0000e+00 - precision: 0.5013\n",
            "Epoch 44/50\n",
            "310/310 [==============================] - 32s 103ms/step - loss: 0.8969 - accuracy: 0.0000e+00 - precision: 0.4994\n",
            "Epoch 45/50\n",
            "310/310 [==============================] - 32s 103ms/step - loss: 0.8776 - accuracy: 0.0000e+00 - precision: 0.4957\n",
            "Epoch 46/50\n",
            "310/310 [==============================] - 32s 103ms/step - loss: 0.8588 - accuracy: 0.0000e+00 - precision: 0.5002\n",
            "Epoch 47/50\n",
            "310/310 [==============================] - 32s 103ms/step - loss: 0.8409 - accuracy: 0.0000e+00 - precision: 0.4935\n",
            "Epoch 48/50\n",
            "310/310 [==============================] - 33s 106ms/step - loss: 0.8233 - accuracy: 0.0000e+00 - precision: 0.4977\n",
            "Epoch 49/50\n",
            "310/310 [==============================] - 32s 103ms/step - loss: 0.8063 - accuracy: 0.0000e+00 - precision: 0.5041\n",
            "Epoch 50/50\n",
            "310/310 [==============================] - 33s 106ms/step - loss: 0.7906 - accuracy: 0.0000e+00 - precision: 0.4988\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1e670e4fe10>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataGen = image.ImageDataGenerator(\n",
        "    rescale = 1.0/255, # transforma pixeis de ate 255 rgb para valores de 0 a 1\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "\n",
        "X_train = dataGen.flow_from_directory(\n",
        "    './PetImages',\n",
        "    target_size=(64,64),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical', # 3(categorical, binary, regression)\n",
        "    subset='training'\n",
        "\n",
        ")\n",
        "\n",
        "X_test = dataGen.flow_from_directory(\n",
        "    './PetImages',\n",
        "    target_size=(64,64),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical', # 3(categorical, binary, regression)\n",
        "    subset='validation'\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    steps_per_epoch = 310,\n",
        "    epochs = 50,\n",
        "    validation_steps = 78,\n",
        "    callbacks= [\n",
        "        # callbacks.EarlyStopping(patience = 4),\n",
        "        callbacks.ModelCheckpoint(\n",
        "            filepath = 'model.{epoch:02d}.h5'\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_k7YkoG6WKD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPsGy81cfymOamOeVdLgQ3n",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
